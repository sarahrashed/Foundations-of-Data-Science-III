{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display_html, display, Math, HTML;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Functions of Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Today we will talk about the properties of functions of random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "````{margin}\n",
    "```{note}\n",
    "This section draws on the excellent notes [here](https://dlsun.github.io/probability/linearity.html).\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LOTUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Imagine we want to study some function $g(\\cdot)$ of a random variable $X$.\n",
    "\n",
    "$g(X)$ could be any expression, eg, $X^2$ or $e^X$ or $\\log X$ or whatever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the expectation of $g(X)$, ie, $E[g(X)]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we think of $g(X)$ as a new random variable, then it would have expectation:\n",
    "    \n",
    "$$ E[g(X)] = \\sum_x g(x) p(x). $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, for example, we can say that\n",
    "\n",
    "$$ E[X^2] = \\sum_x x^2 p(x) $$\n",
    "\n",
    "or \n",
    "\n",
    "$$ E[e^X] = \\sum_x e^x p(x) $$\n",
    "\n",
    "and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So what does LOTUS even stand for? \n",
    "\n",
    "The \"Law of the Unconscious Statistician\" (LOTUS) because it is so often used without even thinking about it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's use LOTUS to build up some more facts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is $E[aX]$ where $a$ is a number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since $aX$ is a function of $X$, we use \n",
    "LOTUS:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "E[aX] &= \\sum_x ax p(x) & \\text{(LOTUS)} \\\\\n",
    "&= a \\sum_x x p(x) & \\text{(factor constant outside the sum)} \\\\\n",
    "&= a E[X] & \\text{(definition of expected value)}. \n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And what is $E[X+b]$ where $b$ is a number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$ \\begin{align*}\n",
    "E[X+b] &= \\sum_x (x + b) p(x) & \\text{(LOTUS)} \\\\\n",
    "&= \\sum_x x p(x) + \\sum_x b p(x) & \\text{(break $(x + b) p(x)$ into $xp(x) + bp(x)$)} \\\\\n",
    "&= \\sum_x x p(x) + b\\sum_x p(x) & \\text{(factor constant outside the sum)} \\\\\n",
    "&= E[X] + b & \\text{(definitions of expected value, p.m.f.)}.\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining the above observations we can conclude that the expected value of $aX+b$, $E[aX+b]$, is equal to $aE[X] + b.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sums of Random Variables\n",
    "\n",
    "With LOTUS, we can now think about sums of random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linearity of Expectation\n",
    "\n",
    "Let $X$ and $Y$ be random variables. Then, _no matter what their joint distribution is_, \n",
    "\n",
    "$$\\begin{equation}\n",
    "E[X+Y] = E[X] + E[Y].\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Proof.__\n",
    "\n",
    "Since $E[X + Y]$ involves two random variables, we have to evaluate the expectation \n",
    "using LOTUS, with $g(x, y) = x + y$. Suppose \n",
    "that the joint distribution of $X$ and $Y$ is $p(x, y)$. Then:\n",
    "    \n",
    "$$\n",
    "\\begin{align*}\n",
    "E[X + Y] &= \\sum_x \\sum_y (x + y) p(x, y) & \\text{(LOTUS)} \\\\\n",
    " &= \\sum_x \\sum_y x p(x, y) + \\sum_x \\sum_y y p(x, y) & \\text{(break $(x + y) p(x, y)$ into $x p(x, y) + y p(x, y)$)} \\\\\n",
    "&= \\sum_x x \\sum_y p(x, y) + \\sum_y y \\sum_x p(x, y) & \\text{(move term outside the inner sum)} \\\\\n",
    "&= \\sum_x x p_X(x) + \\sum_y y p_Y(y) & \\text{(definition of marginal distribution)} \\\\\n",
    "&= E[X] + E[Y] & \\text{(definition of expected value)}.\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In other words, linearity of expectation says that you only need to know the \n",
    "marginal distributions of $X$ and $Y$ to calculate $E[X + Y]$. \n",
    "\n",
    "In particular, it does not matter whether $X$ and $Y$ are independent.  \n",
    "\n",
    "Even if $X$ and $Y$ are correlated, their expectations still add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An important corollary is this:  suppose we have $n$ random variables with the same distribution.\n",
    "    \n",
    "Then \n",
    "\n",
    "$$ E\\left[\\sum_i X_i\\right] = n E[X_1] $$\n",
    "\n",
    "That is, if you have $n$ random variables, and each has mean $\\mu$, then the mean of the sum is $n\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example 1\n",
    "\n",
    "Use the linearity of expectation to calculate the expected value of the Binomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Steps to Solution__\n",
    "\n",
    " 1. Note that a Binomial is the sum of Bernoulli trials.\n",
    " 2. Determine the expected value of a Bernoulli trial.\n",
    " 3. Find expected value of Binomial by linearity of expectation of sum of Bernoulli trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Solution__\n",
    "\n",
    "Expected value of a Bernoulli trial is $p$, and we have $n$ trials, so expected value of a Binomial distribution is $np$. This agrees with what we expected from directly calculating the expected value of a Binomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example 2\n",
    "\n",
    "Suppose two people are playing Roulette. They each first bet on red three times in a row.  (Note that in Roulette, 18 of the 38 numbers are red). The first player leaves, but the second player bets two more times on red. How many more times is player 2 expected to win than player one?\n",
    "\n",
    "Crucially, note that the number of times player 2 wins is not independent from the number of times player 1 wins, because every time player 1 wins, player 2 also wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "````{margin}\n",
    "```{note}\n",
    "Adapted from example 26.2 from https://dlsun.github.io/probability/linearity.html\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Solution__\n",
    "\n",
    "Let $X$ be the number of times player one wins and $Y$ be the number of times player 2 wins, we want to calculate $E[Y-X]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is $X$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$X$ is Binomial with $n=3$ and $p=18/38$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Similarly, $Y$ is Binomial with $n=5$ and $p=18/38$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we calculate $E[Y-X]$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ E[Y−X]=E[Y]+E[−1⋅X]=E[Y]+(−1)E[X]=E[Y]−E[X]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We just showed that the expect value of a Binomail is $np$ so putting it all together: \n",
    "\n",
    "$$E[Y−X]=E[Y]−E[X]=5*18/38−3*18/38=2*18/38 = 18/19$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variance and Covariance\n",
    "\n",
    "Using linearity of expectation we can prove some useful equations for calculating variance and covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Consider $\\operatorname{Cov}(X, Y)$ where $X$ and $Y$ may have any joint distribution. Recall that:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\text{Cov}(X,Y) & =E[(X−E[X])(Y−E[Y])] \\\\\n",
    "\\\\\n",
    "& =E[XY−XE[Y]−E[X]Y+E[X]E[Y]] & \\text{Expansion} \\\\\n",
    "\\\\\n",
    "& =E[XY]−E[XE[Y]]−E[E[X]Y]+ E[E[X]E[Y]]  & \\text{Linearity of Expectation} \\\\\n",
    "\\\\\n",
    "& =E[XY]−E[X]E[Y]−E[X]E[Y]+E[X]E[Y] & \\text{because } E[aX] = aE[X] \\\\\n",
    "\\\\\n",
    "& =E[XY]-E[X]E[Y]\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\operatorname{Cov}(X, Y)=E[XY]−E[X]E[Y]$ is a valuable simplification when computing covariance.  \n",
    "\n",
    "It only requires computing the means of $X$ and $Y$, and $E[XY]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "From this fact we can also conclude that:\n",
    "    \n",
    "$$ \\operatorname{Var}(X) = E[X^2] - E[X]^2 $$\n",
    "\n",
    "One of the most useful results of the linearity of expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variance of  a Sum\n",
    "\n",
    "Let's keep using these facts to explore how the variance of a sum works.\n",
    "\n",
    "Consider $X$ and $Y$ which may have any joint distribution.\n",
    "\n",
    "What is $\\operatorname{Var}(X + Y)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned} \n",
    "\\text{Var}(X + Y) &= E[(X+Y)(X+Y)] - E[(X+Y)]^2 \\ \\ \\ \\text{formula just proved.} \\\\\n",
    "\\\\\n",
    "&= E[X^2 + Y^2 + 2XY] - (E[X] + E[Y])^2  \\\\\n",
    "\\\\\n",
    "&= E[X^2 + Y^2 + 2XY] - (E[X]^2 + E[Y]^2 + 2E[X]E[Y])  \\\\\n",
    "\\\\\n",
    "&= E[X^2] + E[Y^2] + 2E[XY] - E[X]^2 - E[Y]^2 -2E[X]E[Y] \\\\\n",
    "\\\\\n",
    "&= (E[X^2] - E[X]^2) + (E[Y^2] - E[Y]^2) + 2(E[XY] - E[X]E[Y]) \\\\\n",
    "\\\\\n",
    "&= \\operatorname{Var}(X) + \\operatorname{Var}(Y) + 2 \\operatorname{Cov}(X, Y) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we see that when adding random variables, there is a __correction__ to the variance:  if the variables are positively correlated, then the variance of their sum is greater than the sum of their variances.\n",
    "\n",
    "The amount of this correction is twice the covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There's another important way to look at this result: \n",
    "\n",
    "When adding __independent__ random variables, __variances sum.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, consider the case where we are summing $n$ independent random variables, each with mean $\\mu$ and variance $\\sigma^2$.\n",
    "\n",
    "Then the sum has mean $n\\mu$ and variance $n\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of $aX+b$\n",
    "\n",
    "Finally, if $\\operatorname{Var}(X)$ exists, then $\\operatorname{Var}(aX+b) = a^2\\operatorname{Var}(X)$ for constants $a$ and $b$. The proof of this property is outside the scope of this course."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "rise": {
   "scroll": false,
   "theme": "serif",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
